import numpy as np
import tensorflow as tf

sobel_y_kernel = np.array([[-0.5326495, 2.6255102,-0.49177617 ],
                      [-0.5265761, 2.643906, -0.44409272],
                      [-0.55045366, 2.625338, -0.47660905]], dtype='float32').reshape((3, 3, 1, 1))

matrice_test = np.array([[ 
    [0.226, 0.249, 0.269, 0.283, 0.284, 0.330, 0.344, 0.335, 0.360, 0.356, 0.366, 0.374, 0.380],
    [0.202, 0.239, 0.276, 0.291, 0.316, 0.315, 0.340, 0.363, 0.365, 0.363, 0.377, 0.386, 0.388],
    [0.240, 0.259, 0.300, 0.306, 0.307, 0.303, 0.335, 0.345, 0.364, 0.372, 0.376, 0.383, 0.395],
    [0.234, 0.282, 0.289, 0.301, 0.303, 0.328, 0.332, 0.351, 0.361, 0.370, 0.383, 0.385, 0.385],
    [0.216, 0.252, 0.291, 0.305, 0.306, 0.314, 0.340, 0.332, 0.368, 0.372, 0.374, 0.382, 0.375],
    [0.212, 0.272, 0.269, 0.289, 0.308, 0.332, 0.351, 0.351, 0.375, 0.375, 0.366, 0.374, 0.386],
    [0.245, 0.279, 0.288, 0.278, 0.311, 0.325, 0.337, 0.350, 0.357, 0.368, 0.379, 0.375, 0.375],
    [0.239, 0.253, 0.274, 0.300, 0.310, 0.330, 0.336, 0.363, 0.357, 0.357, 0.360, 0.374, 0.382],
    [0.237, 0.284, 0.298, 0.312, 0.325, 0.319, 0.323, 0.335, 0.338, 0.367, 0.373, 0.373, 0.386],
    [0.213, 0.263, 0.272, 0.278, 0.300, 0.328, 0.359, 0.343, 0.362, 0.372, 0.384, 0.394, 0.386],
    [0.226, 0.289, 0.298, 0.297, 0.320, 0.343, 0.335, 0.344, 0.362, 0.377, 0.384, 0.375, 0.379],
    [0.229, 0.272, 0.298, 0.305, 0.297, 0.327, 0.334, 0.350, 0.371, 0.362, 0.372, 0.374, 0.381],
    [0.200, 0.268, 0.291, 0.302, 0.300, 0.305, 0.331, 0.345, 0.358, 0.361, 0.379, 0.379, 0.391],
    [0.219, 0.290, 0.298, 0.304, 0.311, 0.356, 0.412, 0.417, 0.440, 0.424, 0.392, 0.403, 0.390],
    [0.236, 0.286, 0.302, 0.334, 0.355, 0.360, 0.376, 0.366, 0.367, 0.392, 0.397, 0.410, 0.390],
    [0.222, 0.277, 0.309, 0.350, 0.369, 0.401, 0.406, 0.379, 0.356, 0.367, 0.385, 0.397, 0.387],
    [0.222, 0.287, 0.332, 0.367, 0.388, 0.386, 0.377, 0.359, 0.354, 0.380, 0.397, 0.411, 0.394],
    [0.216, 0.287, 0.321, 0.344, 0.370, 0.361, 0.364, 0.359, 0.374, 0.374, 0.395, 0.404, 0.405],
    [0.312, 0.335, 0.356, 0.373, 0.400, 0.395, 0.382, 0.365, 0.363, 0.364, 0.374, 0.391, 0.396],
    [0.363, 0.405, 0.432, 0.394, 0.399, 0.390, 0.378, 0.386, 0.389, 0.383, 0.379, 0.373, 0.390],
    [0.325, 0.397, 0.447, 0.428, 0.411, 0.385, 0.336, 0.307, 0.293, 0.306, 0.313, 0.314, 0.307],
    [0.329, 0.407, 0.443, 0.431, 0.404, 0.381, 0.321, 0.303, 0.292, 0.306, 0.314, 0.328, 0.322],
    [0.328, 0.408, 0.431, 0.422, 0.393, 0.390, 0.330, 0.309, 0.298, 0.307, 0.323, 0.331, 0.314],
    [0.323, 0.401, 0.424, 0.413, 0.374, 0.390, 0.338, 0.308, 0.300, 0.320, 0.338, 0.336, 0.315],
    [0.319, 0.401, 0.420, 0.412, 0.362, 0.388, 0.370, 0.318, 0.308, 0.336, 0.362, 0.350, 0.311],
    [0.312, 0.396, 0.417, 0.406, 0.343, 0.382, 0.387, 0.317, 0.308, 0.370, 0.408, 0.369, 0.307],
    [0.306, 0.371, 0.394, 0.383, 0.306, 0.351, 0.361, 0.321, 0.357, 0.409, 0.385, 0.362, 0.344],
    [0.299, 0.322, 0.360, 0.344, 0.277, 0.312, 0.342, 0.354, 0.384, 0.435, 0.404, 0.402, 0.382],
    [0.301, 0.324, 0.352, 0.344, 0.256, 0.279, 0.315, 0.359, 0.361, 0.441, 0.425, 0.409, 0.388],
    [0.275, 0.306, 0.339, 0.328, 0.260, 0.286, 0.303, 0.359, 0.364, 0.426, 0.424, 0.399, 0.388],
    [0.266, 0.293, 0.321, 0.314, 0.239, 0.265, 0.299, 0.389, 0.378, 0.403, 0.414, 0.399, 0.397],
    [0.248, 0.282, 0.312, 0.306, 0.257, 0.267, 0.289, 0.380, 0.372, 0.418, 0.438, 0.390, 0.383],
    [0.260, 0.291, 0.312, 0.295, 0.244, 0.253, 0.292, 0.372, 0.366, 0.495, 0.469, 0.377, 0.361],
    [0.221, 0.251, 0.285, 0.277, 0.252, 0.256, 0.288, 0.356, 0.406, 0.483, 0.397, 0.349, 0.329],
    [0.211, 0.240, 0.260, 0.262, 0.240, 0.246, 0.283, 0.319, 0.356, 0.388, 0.341, 0.309, 0.315],
    [0.185, 0.216, 0.237, 0.235, 0.201, 0.232, 0.254, 0.280, 0.283, 0.368, 0.339, 0.305, 0.307],
    [0.186, 0.214, 0.227, 0.229, 0.207, 0.229, 0.244, 0.283, 0.280, 0.344, 0.307, 0.285, 0.283],
    [0.167, 0.208, 0.230, 0.231, 0.225, 0.227, 0.245, 0.253, 0.274, 0.346, 0.311, 0.286, 0.293],
    [0.172, 0.213, 0.223, 0.211, 0.212, 0.233, 0.249, 0.261, 0.273, 0.344, 0.308, 0.289, 0.292],
    [0.176, 0.213, 0.211, 0.205, 0.192, 0.220, 0.246, 0.259, 0.279, 0.318, 0.295, 0.284, 0.294],
    [0.166, 0.189, 0.233, 0.218, 0.211, 0.225, 0.237, 0.262, 0.271, 0.309, 0.304, 0.287, 0.297],
    [0.175, 0.175, 0.227, 0.232, 0.219, 0.242, 0.260, 0.263, 0.283, 0.305, 0.302, 0.293, 0.296],
    [0.182, 0.206, 0.243, 0.228, 0.210, 0.245, 0.247, 0.249, 0.270, 0.301, 0.300, 0.302, 0.304],
    [0.148, 0.181, 0.224, 0.222, 0.219, 0.250, 0.246, 0.263, 0.277, 0.295, 0.300, 0.294, 0.289],
    [0.169, 0.180, 0.213, 0.209, 0.231, 0.246, 0.251, 0.262, 0.294, 0.312, 0.296, 0.297, 0.289],
    [0.151, 0.210, 0.213, 0.217, 0.208, 0.234, 0.257, 0.266, 0.288, 0.301, 0.303, 0.293, 0.306],
    [0.145, 0.196, 0.213, 0.217, 0.215, 0.228, 0.258, 0.260, 0.285, 0.293, 0.289, 0.302, 0.306],
    [0.345, 0.337, 0.321, 0.310, 0.305, 0.297, 0.296, 0.297, 0.309, 0.299, 0.295, 0.289, 0.305]
    ]], dtype='float32')

model = tf.keras.Sequential()
model.add(tf.keras.layers.Input(shape=(13, 48, 1)))
conv_layer = tf.keras.layers.Conv2D(1, (3, 3), activation='linear')
conv_layer.build((None, 13, 48, 1))  # Construire la couche pour initialiser les poids
conv_layer.set_weights([sobel_y_kernel,np.zeros(1)])
model.add(conv_layer)
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Flatten())

model.summary()


predictions = model.predict(matrice_test)


# Afficher les pr√©dictions
print(predictions)

# python result maxpool
# 1.4351623 1.5092006 1.6827779 1.7927647 1.8531171
# 1.4610293 1.4964149 1.6819446 1.824388  1.8425461
# 1.3894665 1.514426  1.6984916 1.8157039 1.817189 
# 1.4061226 1.5531721 1.6635859 1.7287267 1.8139575
# 1.4284079 1.5444953 1.6815463 1.8021332 1.866682 
# 1.4641318 1.4938339 1.7788694 1.9376734 1.8654474
# 1.4699063 1.6673434 1.9935402 1.9092808 1.9369704
# 1.5760844 1.8563093 1.9155804 1.7765878 1.9257025
# 1.8433568 1.9411626 1.8598839 1.8389236 1.900948 
# 2.2410128 2.0169377 1.913689  1.7064543 1.7341238
# 2.2229977 2.0936806 1.9618155 1.4729185 1.5965235
# 2.1187403 2.065616  1.9794338 1.4764655 1.8680711
# 2.0478165 2.0242777 1.8911946 1.657004  2.0672297
# 1.8597872 1.8294904 1.6880068 1.7836189 2.241958 
# 1.703715  1.696767  1.4466572 1.9665737 2.1555173
# 1.5911405 1.5529474 1.3573478 2.0049257 2.4469872
# 1.4430205 1.3890563 1.373013  1.7649312 2.4210925
# 1.203466  1.2162166 1.2597997 1.4678004 1.9374384
# 1.1330116 1.0983957 1.2189431 1.3116072 1.8415021
# 1.1484582 1.0711433 1.2171263 1.3323228 1.6863025
# 1.2247491 1.1172471 1.236886  1.3438559 1.5393106
# 1.1749885 1.0589997 1.245384  1.4091115 1.515989 
# 1.25528   1.2118723 1.3379704 1.4581603 1.5012009

# arduino result maxpool
# 1.44   1.51    1.68    1.79    1.85
# 1.46    1.50    1.68    1.82    1.84
# 1.39    1.51    1.70    1.82    1.82
# 1.41    1.55    1.66    1.73    1.81
# 1.43    1.54    1.68    1.80    1.87
# 1.46    1.49    1.78    1.94    1.87
# 1.47    1.67    1.99    1.91    1.94
# 1.58    1.86    1.92    1.78    1.93
# 1.84    1.94    1.86    1.84    1.90
# 2.24    2.02    1.91    1.71    1.73
# 2.22    2.09    1.96    1.47    1.60
# 2.12    2.07    1.98    1.48    1.87
# 2.05    2.02    1.89    1.66    2.07
# 1.86    1.83    1.69    1.78    2.24
# 1.70    1.70    1.45    1.97    2.16
# 1.59    1.55    1.36    2.00    2.45
# 1.44    1.39    1.37    1.76    2.42
# 1.20    1.22    1.26    1.47    1.94
# 1.13    1.10    1.22    1.31    1.84
# 1.15    1.07    1.22    1.33    1.69
# 1.22    1.12    1.24    1.34    1.54
# 1.17    1.06    1.25    1.41    1.52
# 1.26    1.21    1.34    1.46    1.50 

# python result flatten
#  1.4351623 1.5092006 1.6827779 1.7927647 1.8531171 1.4610293 1.4964149
#  1.6819446 1.824388  1.8425461 1.3894665 1.514426  1.6984916 1.8157039
#  1.817189  1.4061226 1.5531721 1.6635859 1.7287267 1.8139575 1.4284079
#  1.5444953 1.6815463 1.8021332 1.866682  1.4641318 1.4938339 1.7788694
#  1.9376734 1.8654474 1.4699063 1.6673434 1.9935402 1.9092808 1.9369704
#  1.5760844 1.8563093 1.9155804 1.7765878 1.9257025 1.8433568 1.9411626
#  1.8598839 1.8389236 1.900948  2.2410128 2.0169377 1.913689  1.7064543
#  1.7341238 2.2229977 2.0936806 1.9618155 1.4729185 1.5965235 2.1187403
#  2.065616  1.9794338 1.4764655 1.8680711 2.0478165 2.0242777 1.8911946
#  1.657004  2.0672297 1.8597872 1.8294904 1.6880068 1.7836189 2.241958
#  1.703715  1.696767  1.4466572 1.9665737 2.1555173 1.5911405 1.5529474
#  1.3573478 2.0049257 2.4469872 1.4430205 1.3890563 1.373013  1.7649312
#  2.4210925 1.203466  1.2162166 1.2597997 1.4678004 1.9374384 1.1330116
#  1.0983957 1.2189431 1.3116072 1.8415021 1.1484582 1.0711433 1.2171263
#  1.3323228 1.6863025 1.2247491 1.1172471 1.236886  1.3438559 1.5393106
#  1.1749885 1.0589997 1.245384  1.4091115 1.515989  1.25528   1.2118723
#  1.3379704 1.4581603 1.5012009 

# arduino result flatten
# 1.44    1.51    1.68    1.79    1.85    1.46    1.50    
# 1.68    1.82    1.84    1.39    1.51    1.70    1.82    
# 1.82    1.41    1.55    1.66    1.73    1.81    1.43    
# 1.54    1.68    1.80    1.87    1.46    1.49    1.78    
# 1.94    1.87    1.47    1.67    1.99    1.91    1.94    
# 1.58    1.86    1.92    1.78    1.93    1.84    1.94    
# 1.86    1.84    1.90    2.24    2.02    1.91    1.71  
# 1.73    2.22    2.09    1.96    1.47    1.60    2.12    
# 2.07    1.98    1.48    1.87    2.05    2.02    1.89    
# 1.66    2.07    1.86    1.83    1.69    1.78    2.24    
# 1.70    1.70    1.45    1.97    2.16    1.59    1.55    
# 1.36    2.00    2.45    1.44    1.39    1.37    1.76    
# 2.42    1.20    1.22    1.26    1.47    1.94    1.13    
# 1.10    1.22    1.31    1.84    1.15    1.07    1.22    
# 1.33    1.69    1.22    1.12    1.24    1.34    1.54    
# 1.17    1.06    1.25    1.41    1.52    1.26    1.21    
# 1.34    1.46    1.50